<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<!-- ###### start inserted header ##### -->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-115940772-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-115940772-1');
</script>

<!-- add the twitter card and open graph tags -->
<meta name="twitter:card" content="summary">
<meta name="twitter:creator" content="@djnavarro">
<meta property="og:url" content="http://compcogscisydney.org/psyr/">
<meta property="og:title" content="R for Psychological Science">
<meta property="og:description" content="An introductory resource">
<meta property="og:image" content="http://compcogscisydney.org/psyr/img/splash_turtle.png">

<!-- ###### end inserted header ##### -->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="mystyle.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 60px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 65px;
  margin-top: -65px;
}

.section h2 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h3 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h4 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h5 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h6 {
  padding-top: 65px;
  margin-top: -65px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">R for Psychological Science</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Core
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="getting-started.html">Getting started</a>
    </li>
    <li>
      <a href="variables.html">Variables</a>
    </li>
    <li>
      <a href="scripts.html">Scripts</a>
    </li>
    <li>
      <a href="packages.html">Packages</a>
    </li>
    <li>
      <a href="workspaces.html">Workspaces</a>
    </li>
    <li>
      <a href="vectors.html">Vectors</a>
    </li>
    <li>
      <a href="loops.html">Loops</a>
    </li>
    <li>
      <a href="branches.html">Branches</a>
    </li>
    <li>
      <a href="functions.html">Functions</a>
    </li>
    <li>
      <a href="programming.html">Programming</a>
    </li>
    <li>
      <a href="file-system.html">File system</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="prelude-to-data.html">Prelude</a>
    </li>
    <li>
      <a href="data-types.html">Data types</a>
    </li>
    <li>
      <a href="describing-data.html">Describing data</a>
    </li>
    <li>
      <a href="visualising-data.html">Visualising data</a>
    </li>
    <li>
      <a href="manipulating-data.html">Manipulating data</a>
    </li>
    <li>
      <a href="working-with-text.html">Text data</a>
    </li>
    <li>
      <a href="import-export.html">Import/export</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Stats
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="probability.html">Probability distributions</a>
    </li>
    <li>
      <a href="introductory-statistics.html">Introductory statistics</a>
    </li>
    <li>
      <a href="intermediate-statistics.html">Intermediate statistics</a>
    </li>
    <li>
      <a href="advanced-statistics.html">Advanced statistics</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    More
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="experiments.html">Experiments</a>
    </li>
    <li>
      <a href="shiny.html">Shiny apps</a>
    </li>
    <li>
      <a href="web-scraping.html">Web scraping</a>
    </li>
    <li>
      <a href="xx-miscellaneous.html">Miscellaneous</a>
    </li>
    <li>
      <a href="backprop.html">Backpropagation networks</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://compcogscisydney.org">compcogscisydney.org</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore"><ol start="19" style="list-style-type: decimal">
<li>Probability distributions</li>
</ol></h1>

</div>


<p>R is often described as a <em>statistical programming environment</em>, because - while it does get used for an incredible variety of purposes - it began its life as a tool for helping statisticians analyse data. As such, it has an unrivalled collection of statistical packages built into it, and I’ll talk a little about some of them later. However, before starting to talk about statistics, it’s very useful to talk about <em>probability distributions</em> and how they are usually implemented in R.</p>
<p>An useful place to start is by considering the difference between probability and statistics. Probability theory concerns “the doctrine of chances”. It’s a branch of mathematics that tells you how often different kinds of events will happen, <em>given</em> some assumptions about the world. Some examples:</p>
<ul>
<li>What are the chances of a fair coin coming up heads 10 times in a row?</li>
<li>If I roll two six sided dice, how likely is it that I’ll roll two sixes?</li>
<li>How likely is it that five cards drawn from a perfectly shuffled deck will all be hearts?</li>
<li>What are the chances that I’ll win the lottery?</li>
</ul>
<p>The critical point is that probabilistic questions start with a known <strong>model</strong> of the world, and we use that model to do some calculations.</p>
<p>What about statistics? Statistical questions work the other way around. In statistics, we do not know the truth about the world. All we have is the data, and it is from the data that we want to learn the truth about the world. Statistical questions tend to look more like these:</p>
<ul>
<li>If I flip a coin 10 times and gets 10 heads, is someone playing a trick on me?</li>
<li>If five cards off the top of the deck are all hearts, how likely is it that the deck was shuffled?</li>
<li>If the lottery commissioner’s spouse wins the lottery, how likely is it that the lottery was rigged?</li>
</ul>
<p>This time around, <em>the only thing we have are data</em>. What I know is that I saw my friend flip the coin 10 times and it came up heads every time. And what I want to <strong>infer</strong> is whether or not I should conclude that what I just saw was actually a fair coin being flipped 10 times in a row, or whether I should suspect that my friend is playing a trick on me. To help me solve this problem I might construct two probabilistic models, one assuming this is a fair coin and the other assuming that the data are a trick, and do some comparison of the two. Viewed this way, the statistical inference problem is to figure out which of these probability models is right. Clearly, the statistical question isn’t the same as the probability question, but they’re connected to one another.</p>
<div id="two-views-of-probability" class="section level2">
<h2><span class="header-section-number">19.1</span> Two views of probability</h2>
<p>Let’s start with a simple question that doesn’t have a simple answer: what is “probability”? It might seem surprising, but while statisticians and mathematicians (mostly) agree on what the rules of probability are, there’s much less of a consensus on what the word really means. In the statistic literature there are (at least) two qualitatively different ideas about how to define the term:</p>
<ul>
<li>The <strong>frequentist view</strong> defines probability as <em>long-run frequency</em>. Suppose we were to try flipping a fair coin, over and over again, and divide the number of heads <span class="math inline">\(n_h\)</span> by the total number of coin flips <span class="math inline">\(n\)</span>, yielding <span class="math inline">\(p_h = n_h/n\)</span> as the observed proportion. The frequentists argue that the only way to meaningfully define the idea of probability is in terms of what happens to this empirically observed proportion a the sample size becomes arbitrarily large (i.e., <span class="math inline">\(n \rightarrow \infty\)</span>). In the long run, the proportion of heads will eventually converge to 50%. There are some subtle technicalities that the mathematicians care about, but qualitatively speaking, that’s how the frequentists define probability. Probability is a “thing in the world”</li>
<li>The <strong>Bayesian view</strong> is often called the subjectivist view. It used to be a minority view among statisticians, but it’s been steadily gaining traction for the last several decades - it’s common enough that it’s no longer an unusual approach to statistics. The most common way of thinking about subjective probability is to define the probability of an event as the <em>degree of belief</em> that a rational agent assigns to that truth of that event. From that perspective, probabilities don’t exist in the world, but rather in the thoughts and assumptions of people and other intelligent beings. In order for this approach to work, we need some way of operationalising “degree of belief”. One way that you can do this is to formalise it in terms of “rational gambling”, though there are many other ways. Suppose that I believe that there’s a 60% probability of rain tomorrow. If someone offers me a bet: if it rains tomorrow, then I win $5, but if it doesn’t rain then I lose $5. Clearly, from my perspective, this is a pretty good bet. On the other hand, if I think that the probability of rain is only 40%, then it’s a bad bet to take. Thus, we can operationalise the notion of a “subjective probability” in terms of what bets I’m willing to accept. Probability, from this perspective, is a “thing in the head”.</li>
</ul>
<p>My personal view is much closer to the Bayesian perspective, but I’m also a pragmatist and I use both Bayesian and frequentist methods in my work. In any case, regardless of which version you prefer, Bayesians and frequentists agree on the core mechanics of probability theory, so the tools for working with probabilities in R are the same regardless of which school of thought you prefer!</p>
</div>
<div id="sampling-from-a-set" class="section level2">
<h2><span class="header-section-number">19.2</span> Sampling from a set</h2>
<p>The <code>sample()</code> function is an extremely useful tool. Suppose I have a set of 10 stimuli that I want to present to people in a random order. For simplicity I’ll label the items using <code>letters</code>:<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<pre class="r"><code>stimuli &lt;- letters[1:10]
stimuli</code></pre>
<pre><code>##  [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot;</code></pre>
<p>To sample them in a random order, all I need to do is this</p>
<pre class="r"><code>shuffled_stimuli &lt;- sample(stimuli)
shuffled_stimuli</code></pre>
<pre><code>##  [1] &quot;g&quot; &quot;d&quot; &quot;h&quot; &quot;j&quot; &quot;e&quot; &quot;c&quot; &quot;f&quot; &quot;b&quot; &quot;a&quot; &quot;i&quot;</code></pre>
<p>Or to do the same thing with piped code…</p>
<pre class="r"><code>shuffled_stimuli &lt;- stimuli %&gt;% sample()
shuffled_stimuli</code></pre>
<pre><code>##  [1] &quot;e&quot; &quot;f&quot; &quot;b&quot; &quot;i&quot; &quot;c&quot; &quot;h&quot; &quot;j&quot; &quot;g&quot; &quot;d&quot; &quot;a&quot;</code></pre>
<p>Notice that the output the second time around <em>isn’t</em> the same a the output the first time. The <code>sample()</code> function uses a pseudo-random number generator to order the items differently every time.</p>
<pre class="r"><code>for(i in 1:5) {
  stimuli %&gt;%
    sample() %&gt;%
    print()
}</code></pre>
<pre><code>##  [1] &quot;h&quot; &quot;c&quot; &quot;a&quot; &quot;i&quot; &quot;d&quot; &quot;g&quot; &quot;f&quot; &quot;j&quot; &quot;e&quot; &quot;b&quot;
##  [1] &quot;j&quot; &quot;g&quot; &quot;c&quot; &quot;a&quot; &quot;b&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;h&quot; &quot;i&quot;
##  [1] &quot;b&quot; &quot;f&quot; &quot;e&quot; &quot;h&quot; &quot;c&quot; &quot;j&quot; &quot;i&quot; &quot;a&quot; &quot;d&quot; &quot;g&quot;
##  [1] &quot;j&quot; &quot;d&quot; &quot;f&quot; &quot;c&quot; &quot;h&quot; &quot;g&quot; &quot;i&quot; &quot;e&quot; &quot;b&quot; &quot;a&quot;
##  [1] &quot;e&quot; &quot;j&quot; &quot;g&quot; &quot;b&quot; &quot;d&quot; &quot;a&quot; &quot;i&quot; &quot;h&quot; &quot;c&quot; &quot;f&quot;</code></pre>
<p>As you can see if you feed in a vector of inputs, the default behaviour is to shuffle all the items. However, the <code>sample()</code> function is flexible. For example, “shuffling all the items” is a special case of “sampling without replacement”. Imagine taking all the stimuli (<em>letters</em>), and then placing them into a jar. To shuffle them into a random order all you need to do is shake the jar (<em>randomise</em>) pull them out of the jar (<em>sample</em>) one at a time until the jar is empty (<em>no replacement</em>).</p>
<p>The jars metaphor is nice because we can extend it. Suppose we only want to pull 6 of the 10 stimuli out of the jar:</p>
<pre class="r"><code>stimuli %&gt;% sample(size = 6)</code></pre>
<pre><code>## [1] &quot;a&quot; &quot;j&quot; &quot;i&quot; &quot;g&quot; &quot;h&quot; &quot;c&quot;</code></pre>
<p>Again, there are no repeats (it is impossible to pull the same item from the jar twice), but we stopped before pulling everything out. Here’s a loop showing you how the randomisation gives different answers every time, but it always follows the constraints of stopping at 6 draws and never draws the same item twice:</p>
<pre class="r"><code>for(i in 1:5) {
  stimuli %&gt;%
    sample(size = 6) %&gt;%
    print()
}</code></pre>
<pre><code>## [1] &quot;a&quot; &quot;g&quot; &quot;j&quot; &quot;i&quot; &quot;b&quot; &quot;h&quot;
## [1] &quot;g&quot; &quot;h&quot; &quot;c&quot; &quot;e&quot; &quot;j&quot; &quot;d&quot;
## [1] &quot;i&quot; &quot;j&quot; &quot;b&quot; &quot;g&quot; &quot;c&quot; &quot;f&quot;
## [1] &quot;i&quot; &quot;c&quot; &quot;f&quot; &quot;h&quot; &quot;j&quot; &quot;e&quot;
## [1] &quot;j&quot; &quot;d&quot; &quot;g&quot; &quot;i&quot; &quot;e&quot; &quot;f&quot;</code></pre>
<p>Another way to extend the jars metaphor is to sample <em>with replacement</em>. In this version of the sampling scheme, every time we pull a letter out of the jar we write it down, but then put it back in the jar so that it becomes possible to sample it again.</p>
<pre class="r"><code>for(i in 1:5) {
  stimuli %&gt;%
    sample(size = 15, replace = TRUE) %&gt;%
    print()
}</code></pre>
<pre><code>##  [1] &quot;i&quot; &quot;f&quot; &quot;d&quot; &quot;g&quot; &quot;a&quot; &quot;a&quot; &quot;f&quot; &quot;g&quot; &quot;e&quot; &quot;a&quot; &quot;h&quot; &quot;g&quot; &quot;b&quot; &quot;j&quot; &quot;d&quot;
##  [1] &quot;h&quot; &quot;a&quot; &quot;e&quot; &quot;c&quot; &quot;i&quot; &quot;h&quot; &quot;d&quot; &quot;f&quot; &quot;j&quot; &quot;j&quot; &quot;d&quot; &quot;e&quot; &quot;i&quot; &quot;e&quot; &quot;j&quot;
##  [1] &quot;f&quot; &quot;j&quot; &quot;a&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;g&quot; &quot;i&quot; &quot;i&quot; &quot;f&quot; &quot;b&quot; &quot;g&quot; &quot;j&quot; &quot;a&quot; &quot;a&quot;
##  [1] &quot;e&quot; &quot;c&quot; &quot;f&quot; &quot;c&quot; &quot;a&quot; &quot;f&quot; &quot;a&quot; &quot;b&quot; &quot;f&quot; &quot;j&quot; &quot;a&quot; &quot;e&quot; &quot;d&quot; &quot;b&quot; &quot;a&quot;
##  [1] &quot;a&quot; &quot;i&quot; &quot;g&quot; &quot;c&quot; &quot;b&quot; &quot;c&quot; &quot;h&quot; &quot;f&quot; &quot;i&quot; &quot;a&quot; &quot;h&quot; &quot;i&quot; &quot;d&quot; &quot;j&quot; &quot;j&quot;</code></pre>
<p>Notice that this time you can produce sequences of stimuli that are longer than the original set (because you’re putting them back in the jar). In this case even though I only had 10 items, each output has 15 samples from that set: there are of course repeats!</p>
<p>A final way we can extend the metaphor is to imagine that some of the letters are written on larger pieces of paper than others: so when you reach into the hat you’re more likely to pull out the larger ones.</p>
<pre class="r"><code>weights &lt;- 1:10 # weight the later letters more!</code></pre>
<p>You can use this when sampling with replacement and without. Here’s what it looks like when sampling without replacement:</p>
<pre class="r"><code>for(i in 1:5) {
  stimuli %&gt;%
    sample(size = 10, replace = FALSE, prob = weights) %&gt;%
    print()
}</code></pre>
<pre><code>##  [1] &quot;f&quot; &quot;e&quot; &quot;j&quot; &quot;g&quot; &quot;c&quot; &quot;i&quot; &quot;b&quot; &quot;h&quot; &quot;a&quot; &quot;d&quot;
##  [1] &quot;e&quot; &quot;j&quot; &quot;d&quot; &quot;c&quot; &quot;h&quot; &quot;i&quot; &quot;g&quot; &quot;f&quot; &quot;b&quot; &quot;a&quot;
##  [1] &quot;f&quot; &quot;j&quot; &quot;i&quot; &quot;g&quot; &quot;b&quot; &quot;h&quot; &quot;e&quot; &quot;c&quot; &quot;d&quot; &quot;a&quot;
##  [1] &quot;j&quot; &quot;i&quot; &quot;d&quot; &quot;g&quot; &quot;f&quot; &quot;h&quot; &quot;c&quot; &quot;b&quot; &quot;e&quot; &quot;a&quot;
##  [1] &quot;g&quot; &quot;h&quot; &quot;a&quot; &quot;e&quot; &quot;c&quot; &quot;f&quot; &quot;j&quot; &quot;d&quot; &quot;i&quot; &quot;b&quot;</code></pre>
<p>So in this output every line shuffles the 10 items, but there’s a definite bias in how the items are ordered! We’re much more likely to start with the later letters than with the early ones!</p>
<p>We can also do this when sampling with replacement:</p>
<pre class="r"><code>for(i in 1:5) {
  stimuli %&gt;%
    sample(size = 10, replace = TRUE, prob = weights) %&gt;%
    print()
}</code></pre>
<pre><code>##  [1] &quot;d&quot; &quot;d&quot; &quot;h&quot; &quot;j&quot; &quot;f&quot; &quot;e&quot; &quot;g&quot; &quot;j&quot; &quot;e&quot; &quot;h&quot;
##  [1] &quot;j&quot; &quot;d&quot; &quot;e&quot; &quot;j&quot; &quot;d&quot; &quot;i&quot; &quot;b&quot; &quot;c&quot; &quot;f&quot; &quot;f&quot;
##  [1] &quot;j&quot; &quot;j&quot; &quot;j&quot; &quot;f&quot; &quot;h&quot; &quot;e&quot; &quot;d&quot; &quot;h&quot; &quot;i&quot; &quot;h&quot;
##  [1] &quot;g&quot; &quot;g&quot; &quot;e&quot; &quot;h&quot; &quot;g&quot; &quot;h&quot; &quot;d&quot; &quot;j&quot; &quot;e&quot; &quot;f&quot;
##  [1] &quot;d&quot; &quot;j&quot; &quot;h&quot; &quot;g&quot; &quot;a&quot; &quot;g&quot; &quot;j&quot; &quot;a&quot; &quot;f&quot; &quot;f&quot;</code></pre>
<p>In this output, you can see that we’re very unlikely to sample the letter “a”.</p>
</div>
<div id="binomial-distribution" class="section level2">
<h2><span class="header-section-number">19.3</span> Binomial distribution</h2>
<p>The <code>sample()</code> function gives you a good feel for how you can take a set of entities (stimuli, participants, outcomes, etc) and do probabilistic operations with them. When doing statistics we sometimes like to abstract away from the simple sampling mechanism and start talking in terms of <em>probability distributions</em>. To see how the abstraction works let’s introduce one of the simplest examples, the <strong>binomial distribution</strong>. Imagine we have a six sided die, in which four sides are coloured <code>&quot;blue&quot;</code> and two sides are coloured <code>&quot;red&quot;</code>. Let’s roll the die 20 times and see what we get:</p>
<pre class="r"><code>die &lt;- c(&quot;blue&quot;,&quot;blue&quot;,&quot;blue&quot;,&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;)
rolls &lt;- sample(die, size = 20, replace = TRUE)
rolls</code></pre>
<pre><code>##  [1] &quot;red&quot;  &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; &quot;red&quot;  &quot;red&quot;  &quot;red&quot;  &quot;blue&quot; &quot;blue&quot;
## [11] &quot;red&quot;  &quot;red&quot;  &quot;blue&quot; &quot;red&quot;  &quot;blue&quot; &quot;red&quot;  &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; &quot;blue&quot;</code></pre>
<p>We can count the number of times the result was <code>&quot;blue&quot;</code>:</p>
<pre class="r"><code>n_blue &lt;- sum(rolls == &quot;blue&quot;)
n_blue</code></pre>
<pre><code>## [1] 12</code></pre>
<p>Of course, there’s nothing stopping us from repeating this exercise several times:</p>
<pre class="r"><code>for(i in 1:5){
  rolls &lt;- die %&gt;% sample(size = 20, replace = TRUE)
  n_blue &lt;- sum(rolls == &quot;blue&quot;)
  print(n_blue)
}</code></pre>
<pre><code>## [1] 14
## [1] 13
## [1] 14
## [1] 11
## [1] 13</code></pre>
<p>In fact, let’s go all out on this. Let’s replicate this tiny experiment 100,000 times because that’s easy to do with R:</p>
<pre class="r"><code>n_replications &lt;- 100000
n_blue &lt;- numeric(length = n_replications)
for(r in 1:n_replications){
  rolls &lt;- die %&gt;% sample(size = 20, replace = TRUE)
  n_blue[r] &lt;- sum(rolls == &quot;blue&quot;)
}
n_blue &lt;- factor(n_blue, levels = 0:20, ordered = TRUE)
frequencies &lt;- table(n_blue)
frequencies</code></pre>
<pre><code>## n_blue
##     0     1     2     3     4     5     6     7     8     9    10    11 
##     0     0     0     1     0    13    74   279   939  2562  5513  9939 
##    12    13    14    15    16    17    18    19    20 
## 14867 18188 18073 14409  9210  4234  1375   297    27</code></pre>
<p>With this particular <code>die</code> the probability of observing a <code>&quot;blue&quot;</code> on any one roll is two-thirds (4 out of 6 sides) and not surprisingly the outcomes of this “roll the die 20 times” experiment tend to be <em>distributed</em> mostly between 12 and 15. I hate looking at tables of numbers, so let’s draw a picture:</p>
<pre class="r"><code>as_tibble(frequencies, n = &quot;count&quot;) %&gt;%
  mutate(n_blue = as.numeric(n_blue)) %&gt;%
  ggplot(aes(x=n_blue, y = count)) +
  geom_col(width = .5) +
  theme_bw()</code></pre>
<p><img src="probability_files/figure-html/unnamed-chunk-16-1.png" width="456" style="display: block; margin: auto;" /></p>
<p>This picture is essentially a visualisation of the <strong>binomial distribution</strong> with success probability <code>prob = 2/3</code>, so it’s worth taking a moment to be explicit about what we’ve done. Every one of our experiments produces an <em>outcome</em> (number of blues) that can be described as <em>one</em> random draw from the binomial distribution. So the 100000 replications of the experiment can be viewed as a set of 100000 numbers sampled from the binomial. R contains a function <code>rbinom()</code> that we can use to do this directly:</p>
<pre class="r"><code>n_blue &lt;- rbinom(n = 100000, size = 20, prob = 2/3)</code></pre>
<p>If we process this set of numbers using the same code, we get an almost identical figure. In fact, because I’m going to reuse this code, let’s write a function:</p>
<pre class="r"><code>plot_samples &lt;- function(x, size = 20) {
  x &lt;- factor(x, levels = 0:size)
  frequencies &lt;- table(x)
  proportion &lt;- frequencies / sum(frequencies)
  pic &lt;- as_tibble(proportion, n = &quot;proportion&quot;) %&gt;%
    mutate(x = as.numeric(x)) %&gt;%
    ggplot(aes(x=x, y = proportion)) +
    geom_col(width = .5) +
    xlab(&quot;outcome value&quot;) +
    ggtitle(sum(frequencies)) +
    ylim(0,.3) +
    theme_bw()
  return(pic)
}</code></pre>
<p>Now call it:</p>
<pre class="r"><code>pic &lt;- plot_samples(n_blue)
plot(pic)</code></pre>
<p><img src="probability_files/figure-html/unnamed-chunk-19-1.png" width="456" style="display: block; margin: auto;" /></p>
<p>Of course, the only reason it looks this nice and smooth is that we replicated the experiment 100000 times. Let’s modify the code so the we start out with a relatively small number of replications and watch it smooth out as it gets larger:</p>
<pre class="r"><code>for(rep in seq(from = 50, to = 10000, by = 50)){
  n_blue &lt;- rbinom(n = rep, size = 20, prob = 2/3)
  pic &lt;- plot_samples(n_blue)
  plot(pic)
}</code></pre>
<video width="450" style="display:block; margin: 0 auto;" controls>
<source src="./img/binomial_samples.webm" type="video/webm"> <source src="./img/binomial_samples.mp4" type="video/mp4"> Your browser does not support the video tag.
</video>
<p><br></p>
<p>If you were typing this at the console, this loop would produce a sequence of plots, but what I’ve done (using some clever features of R Markdown) is wrap it up as an animation. Later on I’ll talk about how to make animations.</p>
<p>A natural question to ask is about the true probility of obtaining each outcome. One way to do it (approximately) would be to generate very large number of samples and then calculate the proportion of times that we end up with (say) 12 out of 20 rolls being blue. However, there’s an easier way. It turns out there is a formula for this:</p>
<p><span class="math display">\[
P(k | \theta, n) = \frac{n!}{k!(n-k)!} \theta^k (1-\theta)^{n-k}
\]</span> where <span class="math inline">\(n! = n \times (n-1) \times (n-2) \times \ldots \times 2 \times 1\)</span> refers to <em>n factorial</em>. For some people it can be pretty jarring to see things written mathematically when you’re used to thinking verbally or in terms of R code, so let’s translate that to a function:</p>
<pre class="r"><code>binomial_prob &lt;- function(k, n, theta) {
  first_bit &lt;- factorial(n) / (factorial(k) * factorial(n - k))
  second_bit &lt;- (theta ^ k) * (1 - theta)^(n - k)
  return(first_bit * second_bit)
}</code></pre>
<p>Of course, R already has a function that does this called <code>dbinom()</code>:</p>
<pre class="r"><code>outcome_value &lt;- 0:20
true_prob &lt;- dbinom(x = outcome_value, size = 20, prob = 2/3)
true_prob</code></pre>
<pre><code>##  [1] 2.867972e-10 1.147189e-08 2.179659e-07 2.615590e-06 2.223252e-05
##  [6] 1.422881e-04 7.114406e-04 2.845762e-03 9.248728e-03 2.466327e-02
## [11] 5.425920e-02 9.865310e-02 1.479796e-01 1.821288e-01 1.821288e-01
## [16] 1.457030e-01 9.106440e-02 4.285383e-02 1.428461e-02 3.007287e-03
## [21] 3.007287e-04</code></pre>
<p>But just to confirm that our function actually does the same thing as the version R provides:</p>
<pre class="r"><code>binomial_prob(k = 13, n = 20, theta = 2/3)</code></pre>
<pre><code>## [1] 0.1821288</code></pre>
<p>Or, since we like pictures so much…</p>
<pre class="r"><code>tibble(outcome_value, true_prob) %&gt;%
  ggplot(aes(x = outcome_value, y = true_prob)) +
  geom_col(width = .5) +
  theme_bw()</code></pre>
<p><img src="probability_files/figure-html/unnamed-chunk-23-1.png" width="456" style="display: block; margin: auto;" /></p>
<p>Cool.</p>
<p>The animation below shows how the binomial distribution changes as we shift the value of <code>prob</code>:</p>
<video width="450" style="display:block; margin: 0 auto;" controls>
<source src="./img/binomial.webm" type="video/webm"> <source src="./img/binomial.mp4" type="video/mp4"> Your browser does not support the video tag.
</video>
<p><br></p>
<p>Here’s another one that shows what happens as we change the <code>size</code> of the experiment.</p>
<video width="450" style="display:block; margin: 0 auto;" controls>
<source src="./img/binomial_size.webm" type="video/webm"> <source src="./img/binomial_size.mp4" type="video/mp4"> Your browser does not support the video tag.
</video>
<p><br></p>
<p>You can see the central limit theorem in action here! As the <code>size</code> gets larger, the shape of the binomial distribution gets progressively closer to normal. Speaking of which…</p>
</div>
<div id="normal-distribution" class="section level2">
<h2><span class="header-section-number">19.4</span> Normal distribution</h2>
<p>The normal distribution is the perhaps the most widely used distribution in statistics, so I should talk about it in some detail. It’s also a good moment to talk about how the tools for working with probability distributions in R are structured. As a rule, any distribution that you want to work with in R will be associated with four separate functions. If I want to work with a normal distribution, for instance, there are four different functions - <code>rnorm</code>, <code>dnorm</code>, <code>pnorm</code> and <code>qnorm</code>. If I want to work with a uniform distribution, the functions are named <code>runif</code>, <code>dunif</code>, <code>punif</code> and <code>qunif</code>. For a binomial distribution, they are <code>rbinom</code>, <code>dbinom</code>, <code>pbinom</code> and <code>qbinom</code>. The four versions are:</p>
<ul>
<li>The <strong>r form</strong> is a random number generator: you can use it to sample <code>n</code> random outcomes from the distribution.</li>
<li>The <strong>d form</strong> computes the probability (or probability density) with which you would observe a particular number <code>x</code> if it is generated from this distribution.</li>
<li>The <strong>p form</strong> is the cumulative distribution function. You specify a particular value <code>q</code>, and it tells you the probability of obtaining an outcome smaller than or equal to <code>q</code>.</li>
<li>The <strong>q form</strong> calculates the quantiles of the distribution. You specify a probability value <code>p</code>, and gives you the corresponding percentile. That is, the value of the variable for which there’s a probability <code>p</code> of obtaining an outcome lower than <code>q</code>.</li>
</ul>
<div id="random-sampling" class="section level3">
<h3><span class="header-section-number">19.4.1</span> Random sampling</h3>
<p>Let’s start with a classic example in the psychological context. By convention, measures of cognitive ability (IQ scores) are designed to have a mean of <span class="math inline">\(\mu = 100\)</span> and a standard deviation of <span class="math inline">\(\sigma = 15\)</span>. The <code>rnorm()</code> function allows us to generate normally distributed numbers:</p>
<pre class="r"><code>iq &lt;- rnorm(n = 10, mean = 100, sd = 15)
iq</code></pre>
<pre><code>##  [1] 102.96236 107.18382 115.74983 102.33999 100.28263 105.77267  87.82983
##  [8] 104.60808 102.62654  96.34584</code></pre>
<p>In a real IQ battery such as the <a href="https://en.wikipedia.org/wiki/Wechsler_Adult_Intelligence_Scale">WAIS</a> you would probably get results rounded to the nearest whole number, so it probably makes more sense to think of this as the data:</p>
<pre class="r"><code>iq &lt;- round(iq)
iq</code></pre>
<pre><code>##  [1] 103 107 116 102 100 106  88 105 103  96</code></pre>
<p>If we draw a quick histogram of this…</p>
<pre class="r"><code>tibble(iq) %&gt;% ggplot(aes(x = iq)) + geom_histogram() + theme_bw()</code></pre>
<p><img src="probability_files/figure-html/unnamed-chunk-26-1.png" width="456" style="display: block; margin: auto;" /></p>
<p>… it’s pretty obvious that you can’t tell much about the distribution. So let’s increase the sample size to 1000:</p>
<pre class="r"><code>iq &lt;- rnorm(n = 1000, mean = 100, sd = 15) %&gt;% round
tibble(iq) %&gt;% ggplot(aes(x = iq)) + geom_histogram() + theme_bw()</code></pre>
<p><img src="probability_files/figure-html/unnamed-chunk-27-1.png" width="456" style="display: block; margin: auto;" /></p>
<p>That looks a lot more like the shape we were expecting!</p>
</div>
<div id="probability-density" class="section level3">
<h3><span class="header-section-number">19.4.2</span> Probability density</h3>
<p>Much like the binomial distribution you can imagine that as the sample size gets larger, this shape will smooth out and it will eventually look like a perfect bell curve. As before there is a formula that describes the probability density:</p>
<p><span class="math display">\[
P(x | \mu, \sigma) = \frac{1}{\sqrt{2\pi} \sigma} \exp \left( -\frac{(x-\mu)^2}{2\sigma^2} \right)
\]</span> Just like with the <code>dbinom</code> function, we could implement our own version of it if we really wanted. But why bother? R already does this and does it in a much more efficient way than you or I would. So let’s cut to the chase and use the <code>dnorm</code> function to do the work:</p>
<pre class="r"><code>iq_score &lt;- 40:160
density &lt;- iq_score %&gt;% dnorm(mean = 100, sd = 15)
density &lt;- ifelse(density &lt; 1e-4, 1e-4, density)

tibble(iq_score, density) %&gt;%
  ggplot(aes(x=iq_score,y=density)) +
  geom_area() +
  theme_bw()</code></pre>
<p><img src="probability_files/figure-html/unnamed-chunk-28-1.png" width="456" style="display: block; margin: auto;" /></p>
<p>You’re probably already familiar with how the parameters of the normal distribution work, but even so it’s nice to look at some pretty animations. In the first one, we can see what happens when we shift the mean (i.e. <span class="math inline">\(\mu\)</span>) of the distribution:</p>
<video width="456"  controls loop>
<source src="probability_files/figure-html/normal_mean.webm" />
</video>
<video width="450" style="display:block; margin: 0 auto;" controls>
<source src="./img/normal_mean.webm" type="video/webm"> <source src="./img/normal_mean.mp4" type="video/mp4"> Your browser does not support the video tag.
</video>
<p><br></p>
<p>Not too surprising. It’s maybe a little more informative to look at what happens when we change the standard deviation <span class="math inline">\(\sigma\)</span>:</p>
<video width="456"  controls loop>
<source src="probability_files/figure-html/normal_std.webm" />
</video>
<video width="450" style="display:block; margin: 0 auto;" controls>
<source src="./img/normal_std.webm" type="video/webm"> <source src="./img/normal_std.mp4" type="video/mp4"> Your browser does not support the video tag.
</video>
<p><br></p>
</div>
<div id="cumulative-probability" class="section level3">
<h3><span class="header-section-number">19.4.3</span> Cumulative probability</h3>
<p>The third tool for working with normal distributions is the <code>pnorm()</code> function, which calculates the <em>cumulative distribution function</em> (CDF) for the normal distribution. The CDF describes the probablity that the value <span class="math inline">\(x\)</span> sampled from the normal distribution will be smaller than a particular quantile <span class="math inline">\(q\)</span>. That’s a little abstract, but suppose our question was to ask the probability that someone has an IQ of 110 or below. We could compute that like this:</p>
<pre class="r"><code>pnorm(q = 110, mean = 100, sd = 15)</code></pre>
<pre><code>## [1] 0.7475075</code></pre>
<p>In other words, if an IQ test has been properly calibrated you’d expect about 75% of people to score 110 or below. As with our other examples we can draw the complete CDF for the distribution like this:</p>
<pre class="r"><code>iq_score &lt;- 40:160
cumulative_probability &lt;- iq_score %&gt;% pnorm(mean = 100, sd = 15)

tibble(iq_score, cumulative_probability) %&gt;%
  ggplot(aes(x = iq_score, y = cumulative_probability)) +
  geom_line() +
  theme_bw()</code></pre>
<p><img src="probability_files/figure-html/unnamed-chunk-30-1.png" width="456" style="display: block; margin: auto;" /></p>
<p>Just because we can, here’s an animation:</p>
<!-- NOTE:
ffmpeg -i normal_cdf.webm normal_cdf.gif -hide_banner
ffmpeg -i normal_cdf.webm normal_cdf.mp4 -hide_banner
-->
<video width="450" style="display:block; margin: 0 auto;" controls>
<source src="./img/normal_cdf.webm" type="video/webm">
<source src="./img/normal_cdf.mp4" type="video/mp4">
<p>Your browser does not support the video tag </video> <br></p>
<p>Hopefully you get the idea!</p>
</div>
<div id="quantiles" class="section level3">
<h3><span class="header-section-number">19.4.4</span> Quantiles</h3>
<p>The quantile function is just the inverse of the cumulative distribution (i.e., x and y axes are swapped):</p>
<pre class="r"><code>cumulative_probability &lt;- seq(from = .01, to = .99, by = .01)
quantile &lt;- cumulative_probability %&gt;% qnorm(mean = 100, sd = 15)

tibble(quantile, cumulative_probability) %&gt;%
  ggplot(aes(x = cumulative_probability, y = quantile)) +
  geom_line() +
  theme_bw()</code></pre>
<p><img src="probability_files/figure-html/unnamed-chunk-31-1.png" width="456" style="display: block; margin: auto;" /></p>
<p>The quantile function can be especially useful for working out critical values. So for example, to work out the value of a standard normal that corresponds to the 2.5% lower tail:</p>
<pre class="r"><code>qnorm(p = .025, mean = 0, sd = 1)</code></pre>
<pre><code>## [1] -1.959964</code></pre>
</div>
</div>
<div id="other-distributions" class="section level2">
<h2><span class="header-section-number">19.5</span> Other distributions</h2>
<p>Not yet written!</p>
<div id="poisson-distribution" class="section level3">
<h3><span class="header-section-number">19.5.1</span> Poisson distribution</h3>
</div>
<div id="log-normal-distribution" class="section level3">
<h3><span class="header-section-number">19.5.2</span> Log-normal distribution</h3>
</div>
<div id="response-time-distribution" class="section level3">
<h3><span class="header-section-number">19.5.3</span> Response time distribution</h3>
</div>
<div id="uniform-distribution" class="section level3">
<h3><span class="header-section-number">19.5.4</span> Uniform distribution</h3>
</div>
<div id="beta-distribution" class="section level3">
<h3><span class="header-section-number">19.5.5</span> Beta distribution</h3>
</div>
<div id="t-distribution" class="section level3">
<h3><span class="header-section-number">19.5.6</span> t distribution</h3>
</div>
<div id="chi-square-distribution" class="section level3">
<h3><span class="header-section-number">19.5.7</span> Chi-square distribution</h3>
</div>
<div id="f-distribution" class="section level3">
<h3><span class="header-section-number">19.5.8</span> F distribution</h3>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The <code>letters</code> vector is a built in vector in R that contains the 26 lower case letters of the English alphabet in canonical order. There is also a <code>LETTERS</code> vector that has the upper case letters.<a href="#fnref1">↩</a></p></li>
</ol>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
