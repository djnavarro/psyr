---
title: "Backpropagation networks"
output:
  html_document:
    includes:
      in_header: header.html    
    toc: true
    toc_float: true
    theme: flatly
    highlight: textmate
    css: mystyle.css
    number_sections: true
    pandoc_args: [
      "--number-offset", 0
    ]
---

```{r,echo=FALSE,message=FALSE,warning=FALSE}
rm(list=objects()) # start with a clean workspace
source("knitr_tweaks.R")
library(tidyverse,quietly = TRUE)
```

Earlier in these notes I used the [Rescorla-Wagner](./programming.html) model of associative learning as an example of how to implement computational models of cognition in R. In this and later sections, I'll expand the dicussion of models to cover a variety of other models in the field. I'll start with the **backpropagation rule** for learning in connectionist networks. 

## Scripts and data set

- The [iris_recode.csv](./data/iris_recode.csv) file contains the classic iris data slightly reorganised as purely numeric data ([here](./scripts/iris_recode.R)) is the script that generated it. 
- The first version of the modelling code implements a simple two-layer backpropagation network for the iris data: [iris_twolayer.R](./scripts/iris_twolayer.R)
- The second version of the code implements the same model, but expressing the learning rules as matrix operations in order to speed up the calculations: [iris_twolayer2.R](./scripts/iris_twolayer2.R)

At the moment the scripts don't do anything other than learn a classification rule. The goal for the full exercise will (eventually) be to examine what the model is learning across the series of "epochs", and consider the relationship between this connectionist network and a probabilistic logistic regression model, but for now it's a bit simpler than that!

In this tutorial we'll only cover a very simple version of a backpropagation network, the two-layer "perceptron" model. There are two versions of the code posted above. The code in the [iris_twolayer.R](./scripts/iris_twolayer.R) script is probably the more intuitive version, as it updates the association weights one at a time, but R code runs much faster when you express the learning rule using matrix operations, which is hwat the [iris_twolayer2.R](./scripts/iris_twolayer2.R) version does. Let's start with a walk through of the more intuitive version... 

## Input and output patterns 

First, let's take a look at the training data. I'm going to use the classic "iris" data set that comes bundled with R, but I've reorganised the data in a form that is a little bit more useful for thinking about the learning problem involved, and expressed it as a numeric matrix. 

```{r}
irises <- read_csv("./data/iris_recode.csv") %>% as.matrix()
```

This data set has columns containing many features. First there are the *input features*, which consist of two features relating to the petals, two features relating to the sepal, and a context feature that is 1 for every flower. Additionally, there are three binary valued *output features* corresponding to the species of each flower, dummy coded so that only the correct species has value 1 and the incorrect species have value 0. Here are the names:

```{r}
input_names <- c("sepal_length", "sepal_width", "petal_length", "petal_width", "context")
output_names <- c("species_setosa", "species_versicolor", "species_virginica")
```

So for the first flower, the network would be given this pattern as input:

```{r}
input <- irises[1, input_names]
input
```

and we need to train it to produce this *target pattern* as the output:

```{r}
target <- irises[1, output_names]
target 
```

## Connection weights between input and output

In its simplest form we can describe the knowledge possessed by our network as a set of associative strengths between every input feature and every output feature. In that sense we can think of it as a generalisation of how the Rescorla-Wagner model represents knowledge:

```{r}
n_input <- length(input_names)
n_output <- length(output_names)
n_weights <- n_input * n_output
```

So what we'll do is create a *weight matrix* that sets the initial associative strength to zero, with a tiny bit of random noise added to each of these associative weights:

```{r}
weight <- matrix(
  data = rnorm(n_weights) *.01,
  nrow = n_input,
  ncol = n_output,
  dimnames = list(input_names, output_names)
)
weight
```

Here's the network we want to code:

![](./img/backprop_irises.png)


While we're at it, store a copy for later:

```{r}
old_weight <- weight
```

## Making predictions

In the Rescorla-Wagner model, when the learner is shown a compound stimulus with elements A and B with individual associative strengths $v_A$ and $v_B$, the association strength for the compound AB is assumed to be additive $v_{AB} = v_{A} + v_{B}$. We could do this for our backpropagation network too, but it is much more common to assume a logistic activation function. So we'll need to define this activation function:

```{r}
logit <- function(x){
  y <- 1/(1 + exp(-x))
  return(y)
}
```

So what we do is first take the `sum` of the inputs and then pass them through our new `logit` function. So let's say we want to compute the strength associated with the first species:

```{r}
output_1 <- sum(input * weight[,1]) %>% logit() 
output_1
```

More generally though we can loop over the three species:

```{r}
# initialise the output nodes at zero
output <- rep(0, n_output)
names(output) <- output_names

# feed forward to every output node by taking a weighted sum of
# the inputs and passing it through a logit function
for(o in 1:n_output) {
  output[o] <- sum(input * weight[,o]) %>% logit() 
}

# print the result
output
```

As you can see, initially the model has no knowledge at all! It's predicting a value of about 0.5 for every category!

## Learning from error

The prediction error is very familiar:

```{r}
prediction_error <- target - output
prediction_error
```

Here is the code implementing the learning rule. What we're doing is looping over every weight in the network, and then adjusting the strength proportional to the prediction error:

```{r}
learning_rate <- .1

# for each of the weights connecting to an output node...
for(o in 1:n_output) {
  for(i in 1:n_input) {
    
    # associative learning for this weight scales in a manner that depends on
    # both the input value and output value. this is similar to the way that
    # Rescorla-Wagner has CS scaling (alpha) and US scaling (beta) parameters
    # but the specifics are slightly different (Equation 5 & 6 in the paper)
    scale_io <- input[i] * output[o] * (1-output[o]) 
    
    # adjust the weights proportional to the error and the scaling (Equation 8)
    weight[i,o] <- weight[i,o] + (prediction_error[o] * scale_io * learning_rate)
    
  }
}
```

(Let's not worry too much about the `scale_io` factor for now). So let's look at the input, output, target, and prediction error:

```{r}
input
output
target
prediction_error
```

Now let's look at how the weights changed:

```{r}
weight - old_weight
```

Not surprisingly everything to setosa has gone up and the others down. But notice the scale! 

## Visualising the learning

```{r backproplearn, fig.show='animate', interval=0.1, cache=TRUE, message=FALSE, echo=FALSE, eval = TRUE}

# packages
library(magrittr)
library(readr)
library(dplyr)
library(ggplot2)

# read the irises data as a numeric matrix
iris_file <- here::here("data", "iris_recode.csv")
irises <- iris_file %>% read_csv() %>% as.matrix()

# specify which features are inputs & targets
input_names <- c("sepal_length", "sepal_width", "petal_length", "petal_width", "context")
output_names <- c("species_setosa", "species_versicolor", "species_virginica")

# the logit squashing function
logit <- function(x){
  y <- 1/(1 + exp(-x))
  return(y)
}

# count the number of inputs, target, cases & weights
n_input <- length(input_names)
n_output <- length(output_names)
n_cases <- dim(irises)[1]
n_weights <- n_input * n_output

# parameters
n_epochs <- 100 # number of training epochs
learning_rate <- .1 # learning rate

# create a weight matrix with all weights set to 0
# plus a tiny amount of random noise to break symmetry
weight <- matrix(
  data = rnorm(n_weights) *.01,
  nrow = n_input,
  ncol = n_output,
  dimnames = list(input_names, output_names)
)

# create a matrix that keeps track of the sum squared prediction
# error at the end of every trial, across every epoch
sse <- matrix(NA, n_cases, n_epochs)

for(epoch in 1:n_epochs){
  
  pic <- weight %>% 
    as.tibble() %>%
    mutate(feature = c("SL","SW","PL","PW","C")) %>%
    gather(key = "species", value = "strength", 
           species_setosa, species_versicolor, species_virginica) %>%
    ggplot(aes(x = feature, y = strength)) +
    facet_wrap(~species) +
    geom_bar(stat="identity") +
    ylim(-4.5,4.5) + 
    ggtitle(paste("Epoch:", epoch-1))
  plot(pic)
  
  
  # to prevent weirdness, shuffle the order of trials 
  # at the start of each epoch
  trial_order <- sample(1:n_cases)
  
  for(case in 1:n_cases) {
    
    # which stimulus is this?
    stimulus_id <- trial_order[case]
    
    # send the iris features to the input nodes
    input <- irises[stimulus_id, input_names]
    
    # feed forward expressed as matrix operation
    output <- (input %*% weight) %>% logit()
    
    # feedback: show the model the true "target" pattern
    target <- irises[stimulus_id, output_names]
    
    # error gradient at the output nodes
    prediction_error <- target - output
    
    # store the sum squared error for later
    sse[stimulus_id, epoch] <- sum(prediction_error^2)
    
    # matrix algebra form
    gradient <- input %*% (prediction_error * output * (1-output))
    weight <- weight + learning_rate * gradient
    
  }
  
}
```


## Resources

- The Rumelhart et al (1986) paper cached for teaching purposes [here](http://compcogscisydney.org/mm/2018/Rumelhart1986.pdf)
- A very good, but somewhat technical [summary](http://neuralnetworksanddeeplearning.com/chap2.html) of backpropagation by Michael Nielsen
- Really nice [resources](http://neuroplausible.com/connectionism) in Python by Olivia Guest.


